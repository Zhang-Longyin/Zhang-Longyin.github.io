<!DOCTYPE html>
<html>
<head>
  <title>My Portfolio</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  <style>
    body {
      background-color: #f2f2f2;
    }

    .container {
      padding: 50px;
      font-size: 12px;
    }

    h1 {
      font-size: 36px;
    }

    h2 {
      font-size: 24px;
    }

    p {
      font-size: 16px;
    }

    ul {
      list-style-type: none;
      padding: 0;
    }

    a {
      text-decoration: none;
      color: #000;
    }

    .btn {
      background-color: #000;
      color: #fff;
      padding: 10px 20px;
      border-radius: 5px;
    }
  </style>
</head>
<body>
  <div class="container">
    <div class="row">
      <div class="col-md-8">
        <h1>Research</h1>
        <p>
          <b>Longyin Zhang</b>, Bowei Zou, Jacintha Wee Yun Yi, Ai Ti Aw. Comprehensive Abstractive Comment Summarization with Dynamic Clustering and Chain of Thought. In Proceedings of ACL 2024. <br>
          <b>Longyin Zhang</b>, Bowei Zou, Ai Ti Aw. Empowering Tree-structured Entailment Reasoning: Rhetorical Perception and LLM-driven Interpretability. In Proceedings of LREC-COLING 2024. <br>
          <b>Longyin Zhang</b>, Xin Tan, Peifeng Li, Fang Kong, and Guodong Zhou. Top-Down Text-­Level Discourse Rhetorical Structure Parsing with Bidirectional Representation Learning. In Journal of Computer Science and Technology (JCST). 2023. <br>
          Yuqing Xing, <b>Longyin Zhang</b>, Fang Kong, Guodong Zhou. Discourse Parsing Enhanced by Discourse Dependence Perception. In Proceedings of AACL 2022. <br>
          Xin Tan, <b>Longyin Zhang</b>, Fang Kong, and Guodong Zhou. Towards Discourse-aware Document­level Neural Machine Translation. IJCAI 2022, pages 4383­4389. <br>
          <b>Longyin Zhang</b>, Fang Kong, and Guodong Zhou. Adversarial Learning for Discourse Rhetorical Structure Parsing. In Proceedings of ACL 2021, pages 3946­-3957. <br>
          <b>Longyin Zhang</b>, Yuqing Xing, Fang Kong, Peifeng Li, and Guodong Zhou. A Top­-Down Neural Architecture towards Text­Level Parsing of Discourse Rhetorical Structure. In Proceedings of ACL 2020, pages 6386­6395. <br>
          <b>Longyin Zhang</b>, Xin Tan, Fang Kong, and Guodong Zhou. EDTC: A Corpus for Discourse­Level Topic Chain Parsing. In Proceedings of EMNLP2021 findings, pages 1304­1312. <br>
          Xin Tan, <b>Longyin Zhang</b>, and Guodong Zhou. Coupling Context Modeling with Zero Pronoun Recovering for Document­Level Natural Language Generation. EMNLP 2021, pages 2530­2540. <br>
          <b>Longyin Zhang</b>, Xin Tan, Fang Kong, and Guodong Zhou. Self­-trained and Self-­purified Data Augmentation for RST Discourse Parsing. <br>
          <b>Longyin Zhang</b>, Fang Kong, and Guodong Zhou. Syntax­Guided Sequence to Sequence Modeling for Discourse Segmentation. In Proceedings of NLPCC 2020, pages 95­107. <br>
          <b>Longyin Zhang</b>, Xin Tan, Fang Kong, and Guodong Zhou. A Recursive Information Flow Gated Model for RST­style Text­level Discourse Parsing. In Proceedings of NLPCC 2019, pages 231­241. 2019. <br>
          <b>Longyin Zhang</b>, Cheng Sun, Xin Tan, and Fang Kong. RST Discourse Parsing With Tree­Structured Neural Networks. In Proceedings of CWMT, Communications in Computer and Information Science, vol 954, pages 15­26. 2019. <br>
          Jinfeng Wang, <b>Longyin Zhang</b>, and Fang Kong. Multi­Level Cohesion Information Modeling for Better Written and Dialogue Discourse Parsing. NLPCC 2021, pages 40­52. <br>
          戴倩雯, <b>张龙印</b>, 孔芳. 融合依存关系和篇章修辞关系的事件时序关系识别. 模式识别与人工智能, 32(12):1100­1106. 2019. <br>
          Xin Tan, <b>Longyin Zhang</b>, Deyi Xiong, and Guodong Zhou. Hierarchical Modeling of Global Context for Document­Level Neural Machine Translation. EMNLP 2019, pages 1576­1585. <br>
          谭新, 邝少辉, <b>张龙印</b>, 熊德意. 融入汉字笔画序列的神经机器翻译. 厦门大学学报（自然科学版）, 58(2):164­169. 2019. <br>
        </p>
      </div>
      <div class="col-md-4">
        <img src="zhanglongyin_img.jpeg" alt="My Photo" height="200">
        <h2>Portfolio</h2>
        <p> Longyin Zhang received his Ph.D. from the NLP Research Group of Soochow University under the supervision of Prof. Fang Kong & Prof. Guodong Zhou. He is currently a scientist researcher at <b>A*STAR</b> Institute for Infocomm Research (<b>I2R</b>).</p>
        <p> He has over seven years of NLP research experience in Discourse Analysis, Entailment Reasoning, and Social Media Text Summarization. He has participated in some collaborative research projects between I2R and key parts of Singapore and has participated in guiding students from NUS and NTU to carry out data analysis work. </p>
        <p> He is looking for research collaboration with researchers worldwide, especially on discourse-based NLP applications. </p>
        <p>Email: zhangly@i2r.a-star.edu.sg</p>
      </div>
    </div>
  </div>
</body>
</html>
